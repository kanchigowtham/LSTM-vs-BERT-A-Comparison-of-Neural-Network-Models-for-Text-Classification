{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Loading necessary library\n",
        "\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertForSequenceClassification"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:00:10.653910Z",
          "iopub.execute_input": "2023-03-31T22:00:10.654647Z",
          "iopub.status.idle": "2023-03-31T22:00:19.196077Z",
          "shell.execute_reply.started": "2023-03-31T22:00:10.654556Z",
          "shell.execute_reply": "2023-03-31T22:00:19.195070Z"
        },
        "trusted": true,
        "id": "G16_yVdM4rm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading imbd review dataset from tensorflow dataset\n",
        "\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "          as_supervised=True,with_info=True)\n",
        "clear_output()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:00:19.197730Z",
          "iopub.execute_input": "2023-03-31T22:00:19.198019Z",
          "iopub.status.idle": "2023-03-31T22:01:23.595808Z",
          "shell.execute_reply.started": "2023-03-31T22:00:19.197982Z",
          "shell.execute_reply": "2023-03-31T22:01:23.595114Z"
        },
        "trusted": true,
        "id": "QKfbS6MO4rm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the tokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "clear_output()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:01:23.599706Z",
          "iopub.execute_input": "2023-03-31T22:01:23.601743Z",
          "iopub.status.idle": "2023-03-31T22:01:30.593523Z",
          "shell.execute_reply.started": "2023-03-31T22:01:23.601654Z",
          "shell.execute_reply": "2023-03-31T22:01:30.592631Z"
        },
        "trusted": true,
        "id": "cjiO4Jz54rm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The encode_plus  function of the tokenizer class will tokenize the raw input, add the special tokens, and pad the vector to a size equal to max length (that we can set)."
      ],
      "metadata": {
        "id": "iS2XJOaj4rm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_example_to_feature(review):\n",
        "    return tokenizer.encode_plus(review,\n",
        "                add_special_tokens = True, # add [CLS], [SEP]\n",
        "                max_length = max_length, # max length of the text that can go to BERT\n",
        "                pad_to_max_length = True, # add [PAD] tokens\n",
        "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:01:30.595781Z",
          "iopub.execute_input": "2023-03-31T22:01:30.596088Z",
          "iopub.status.idle": "2023-03-31T22:01:30.600269Z",
          "shell.execute_reply.started": "2023-03-31T22:01:30.596049Z",
          "shell.execute_reply": "2023-03-31T22:01:30.599550Z"
        },
        "trusted": true,
        "id": "xfHO11294rnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can be up to 512 for BERT\n",
        "max_length = 512\n",
        "batch_size = 6"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:01:30.601507Z",
          "iopub.execute_input": "2023-03-31T22:01:30.601929Z",
          "iopub.status.idle": "2023-03-31T22:01:30.611647Z",
          "shell.execute_reply.started": "2023-03-31T22:01:30.601893Z",
          "shell.execute_reply": "2023-03-31T22:01:30.610894Z"
        },
        "trusted": true,
        "id": "B8jbVieR4rnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:01:30.613758Z",
          "iopub.execute_input": "2023-03-31T22:01:30.614189Z",
          "iopub.status.idle": "2023-03-31T22:01:30.622662Z",
          "shell.execute_reply.started": "2023-03-31T22:01:30.614153Z",
          "shell.execute_reply": "2023-03-31T22:01:30.621812Z"
        },
        "trusted": true,
        "id": "AundFzmU4rnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "def encode_examples(ds, limit=-1):\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    label_list = []\n",
        "    if (limit > 0):\n",
        "        ds = ds.take(limit)\n",
        "    for review, label in tfds.as_numpy(ds):\n",
        "        bert_input = convert_example_to_feature(review.decode())\n",
        "        input_ids_list.append(bert_input['input_ids'])\n",
        "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append([label])\n",
        "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:01:30.623972Z",
          "iopub.execute_input": "2023-03-31T22:01:30.624278Z",
          "iopub.status.idle": "2023-03-31T22:01:30.633150Z",
          "shell.execute_reply.started": "2023-03-31T22:01:30.624245Z",
          "shell.execute_reply": "2023-03-31T22:01:30.632415Z"
        },
        "trusted": true,
        "id": "lD24vPhW4rnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train dataset\n",
        "start=time.time()\n",
        "ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n",
        "print(\"Done with Training Dataset\",time.time()-start)\n",
        "# test dataset\n",
        "start=time.time()\n",
        "ds_test_encoded = encode_examples(ds_test).batch(batch_size)\n",
        "print(\"Done with Testing Dataset\",time.time()-start)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:01:30.634662Z",
          "iopub.execute_input": "2023-03-31T22:01:30.634935Z",
          "iopub.status.idle": "2023-03-31T22:11:35.755841Z",
          "shell.execute_reply.started": "2023-03-31T22:01:30.634901Z",
          "shell.execute_reply": "2023-03-31T22:11:35.754906Z"
        },
        "trusted": true,
        "id": "VA5eY3vY4rnB",
        "outputId": "ae28567e-8063-44b6-b621-fe608455bccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Done with Training Dataset 303.64051127433777\nDone with Testing Dataset 301.4707510471344\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
        "learning_rate = 2e-5\n",
        "# we will do just 1 epoch, though multiple epochs might be better as long \n",
        "# as we will not overfit the model\n",
        "number_of_epochs = 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:11:35.757403Z",
          "iopub.execute_input": "2023-03-31T22:11:35.757681Z",
          "iopub.status.idle": "2023-03-31T22:11:35.761785Z",
          "shell.execute_reply.started": "2023-03-31T22:11:35.757641Z",
          "shell.execute_reply": "2023-03-31T22:11:35.760893Z"
        },
        "trusted": true,
        "id": "To2Ctbl44rnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model initialization\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:11:35.764640Z",
          "iopub.execute_input": "2023-03-31T22:11:35.765102Z",
          "iopub.status.idle": "2023-03-31T22:11:49.277007Z",
          "shell.execute_reply.started": "2023-03-31T22:11:35.765063Z",
          "shell.execute_reply": "2023-03-31T22:11:49.276257Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "15bde6e28f6b4647a19b42af64208c58"
          ]
        },
        "id": "oA7FxOry4rnC",
        "outputId": "669b48dd-a009-4b03-be94-a22a1bfe8799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15bde6e28f6b4647a19b42af64208c58"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choosing Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:11:49.278342Z",
          "iopub.execute_input": "2023-03-31T22:11:49.279113Z",
          "iopub.status.idle": "2023-03-31T22:11:49.298835Z",
          "shell.execute_reply.started": "2023-03-31T22:11:49.279069Z",
          "shell.execute_reply": "2023-03-31T22:11:49.298041Z"
        },
        "trusted": true,
        "id": "OMIegVnF4rnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:11:49.300014Z",
          "iopub.execute_input": "2023-03-31T22:11:49.301832Z",
          "iopub.status.idle": "2023-03-31T22:54:32.762446Z",
          "shell.execute_reply.started": "2023-03-31T22:11:49.301789Z",
          "shell.execute_reply": "2023-03-31T22:54:32.761634Z"
        },
        "trusted": true,
        "id": "ztNGkCzM4rnD",
        "outputId": "b2dec04b-e43a-449a-ebca-aa9c6a252b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "4167/4167 [==============================] - 2533s 603ms/step - loss: 0.2408 - accuracy: 0.9000 - val_loss: 0.1834 - val_accuracy: 0.9299\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on Random Sample"
      ],
      "metadata": {
        "id": "sTpC3GmX4rnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"This is a really waste movie. I won't watch again\"\n",
        "\n",
        "predict_input = tokenizer.encode(test_sentence,truncation=True,padding=True,return_tensors=\"tf\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:55:18.529187Z",
          "iopub.execute_input": "2023-03-31T22:55:18.529459Z",
          "iopub.status.idle": "2023-03-31T22:55:18.534890Z",
          "shell.execute_reply.started": "2023-03-31T22:55:18.529429Z",
          "shell.execute_reply": "2023-03-31T22:55:18.534032Z"
        },
        "trusted": true,
        "id": "nvrM4AvN4rnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_output = model.predict(predict_input)[0]\n",
        "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
        "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
        "label = tf.argmax(tf_prediction, axis=1)\n",
        "label = label.numpy()\n",
        "print(labels[label[0]])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-31T22:55:20.419149Z",
          "iopub.execute_input": "2023-03-31T22:55:20.419425Z",
          "iopub.status.idle": "2023-03-31T22:55:23.144708Z",
          "shell.execute_reply.started": "2023-03-31T22:55:20.419397Z",
          "shell.execute_reply": "2023-03-31T22:55:23.143939Z"
        },
        "trusted": true,
        "id": "wYOwf8Fg4rnD",
        "outputId": "646c1d7d-1dfc-432f-e541-4984cdb2a801"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Negative\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}